{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to make tokenizers consistent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Literal\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from io_utils import load_model_and_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ids = [\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"qwen/Qwen2-7B-Instruct\",\n",
    "    \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"cais/zephyr_7b_r2d2\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    'lmsys/vicuna-13b-v1.5'\n",
    "]\n",
    "class ModelConfig:\n",
    "    def __init__(self, id, tokenizer_id, short_name, developer_name, compile, trust_remote_code, dtype, chat_template):\n",
    "        self.id = id\n",
    "        self.tokenizer_id = tokenizer_id\n",
    "        self.short_name = short_name\n",
    "        self.developer_name = developer_name\n",
    "        self.compile = compile\n",
    "        self.trust_remote_code = trust_remote_code\n",
    "        self.dtype = dtype\n",
    "        self.chat_template = chat_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990a3f18697e4400bd18376a88838f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/staff-ssd/beyer/miniconda3/envs/std/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/nfs/staff-ssd/beyer/miniconda3/envs/std/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/nfs/staff-ssd/beyer/miniconda3/envs/std/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/nfs/staff-ssd/beyer/miniconda3/envs/std/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(\n",
    "    'lmsys/vicuna-13b-v1.5',\n",
    "    ModelConfig(\n",
    "        id=\"lmsys/vicuna-13b-v1.5\",  # built-in template is better bc no <bos>, but fixed\n",
    "        tokenizer_id=\"lmsys/vicuna-13b-v1.5\",\n",
    "        short_name=\"Phi\",\n",
    "        developer_name=\"ContinuousAT\",\n",
    "        compile=False,\n",
    "        trust_remote_code=False,\n",
    "        dtype=\"bfloat16\",\n",
    "        chat_template=\"vicuna\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model, tokenizer\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making chat templates equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration instances for each model\n",
    "models = [\n",
    "    ModelConfig(\n",
    "        id=\"google/gemma-2-2b-it\",      # more correct with template, but doesnt matter with manual <bos> removal\n",
    "        tokenizer_id=\"google/gemma-2-2b-it\",\n",
    "        short_name=\"Gemma\",\n",
    "        developer_name=\"Google\",\n",
    "        compile=False,\n",
    "        trust_remote_code=False,\n",
    "        dtype=\"bfloat16\",\n",
    "        chat_template=\"gemma-it\"\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        id=\"mistralai/Mistral-7B-Instruct-v0.3\",    # better with template (adds system role + space before [/INST])\n",
    "        tokenizer_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "        short_name=\"Mistral\",\n",
    "        developer_name=\"Mistral AI\",\n",
    "        compile=False,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",\n",
    "        chat_template=\"mistral-instruct\"\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",    # built-in template is better bc of default system message only, fixed it.\n",
    "        tokenizer_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        short_name=\"Llama\",\n",
    "        developer_name=\"Meta\",\n",
    "        compile=False,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",\n",
    "        chat_template=\"llama-3-instruct\"\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        id=\"qwen/Qwen2-7B-Instruct\",   # identical\n",
    "        tokenizer_id=\"qwen/Qwen2-7B-Instruct\",\n",
    "        short_name=\"Qwen2\",\n",
    "        developer_name=\"Alibaba\",\n",
    "        compile=False,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",\n",
    "        chat_template=None\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        id=\"HuggingFaceH4/zephyr-7b-beta\",   # identical\n",
    "        tokenizer_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "        short_name=\"Zephyr\",\n",
    "        developer_name=\"Hugging Face\",\n",
    "        compile=False,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",\n",
    "        chat_template=\"zephyr\"\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        id=\"meta-llama/Llama-2-7b-chat-hf\",    # identical\n",
    "        tokenizer_id=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        short_name=\"Llama\",\n",
    "        developer_name=\"Meta\",\n",
    "        compile=False,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",\n",
    "        chat_template=\"llama-2-chat\"\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        id=\"ContinuousAT/Llama-2-7B-CAT\",     # identical\n",
    "        tokenizer_id=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "        short_name=\"Llama\",\n",
    "        developer_name=\"ContinuousAT\",\n",
    "        compile=False,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",\n",
    "        chat_template=\"llama-2-chat\",\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        id=\"ContinuousAT/Zephyr-CAT\",         # identical\n",
    "        tokenizer_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "        short_name=\"Zephyr\",\n",
    "        developer_name=\"ContinuousAT\",\n",
    "        compile=False,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",\n",
    "        chat_template=\"zephyr\",\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        id=\"ContinuousAT/Phi-CAT\",          # built-in template is better bc no <bos>, but fixed\n",
    "        tokenizer_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "        short_name=\"Phi\",\n",
    "        developer_name=\"ContinuousAT\",\n",
    "        compile=False,\n",
    "        trust_remote_code=False,\n",
    "        dtype=\"bfloat16\",\n",
    "        chat_template=\"phi-3\"\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        id=\"microsoft/Phi-3-mini-4k-instruct\",              # built-in template is better bc no <bos>, but fixed\n",
    "        tokenizer_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "        short_name=\"Phi\",\n",
    "        developer_name=\"ContinuousAT\",\n",
    "        compile=False,\n",
    "        trust_remote_code=False,\n",
    "        dtype=\"bfloat16\",\n",
    "        chat_template=\"phi-3\"\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        id=\"cais/zephyr_7b_r2d2\",         # identical\n",
    "        tokenizer_id=\"cais/zephyr_7b_r2d2\",\n",
    "        short_name=\"R2D2\",\n",
    "        developer_name=\"ContinuousAT\",\n",
    "        compile=False,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",\n",
    "        chat_template=\"zephyr\"\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        id=\"GraySwanAI/Llama-3-8B-Instruct-RR\",           # identical\n",
    "        tokenizer_id=\"GraySwanAI/Llama-3-8B-Instruct-RR\",\n",
    "        short_name=\"Llama\",\n",
    "        developer_name=\"GraySwanAI\",\n",
    "        compile=False,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",\n",
    "        chat_template=\"llama-3-instruct\"\n",
    "    ),\n",
    "    ModelConfig(\n",
    "        id=\"GraySwanAI/Mistral-7B-Instruct-RR\",           # identical\n",
    "        tokenizer_id=\"GraySwanAI/Mistral-7B-Instruct-RR\",\n",
    "        short_name=\"Mistral\",\n",
    "        developer_name=\"GraySwanAI\",\n",
    "        compile=False,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",\n",
    "        chat_template=\"mistral-instruct\"\n",
    "    ),\n",
    "    ModelConfig(      # identical                        # only possible with chat_template\n",
    "        id=\"lmsys/vicuna-13b-v1.5\",\n",
    "        tokenizer_id=\"lmsys/vicuna-13b-v1.5\",\n",
    "        short_name=\"Vicuna\",\n",
    "        developer_name=\"lmsys\",\n",
    "        compile=False,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"bfloat16\",\n",
    "        chat_template=\"vicuna\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(tokenizer_id, model_params):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_params.tokenizer_id,\n",
    "        trust_remote_code=True,\n",
    "        truncation_side='left',\n",
    "        truncation=True,\n",
    "        padding_side=\"left\"\n",
    "    )\n",
    "\n",
    "    match tokenizer_id.lower():\n",
    "        case path if \"oasst-sft-6-llama-30b\" in path:\n",
    "            tokenizer.bos_token_id = 1\n",
    "            tokenizer.unk_token_id = 0\n",
    "        case path if \"guanaco\" in path:\n",
    "            tokenizer.eos_token_id = 2\n",
    "            tokenizer.unk_token_id = 0\n",
    "        case path if \"llama-2\" in path:\n",
    "            tokenizer.pad_token = tokenizer.unk_token\n",
    "            tokenizer.model_max_length = 4096\n",
    "        case path if \"vicuna\" in path:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        case path if \"gemma-2\" in path:\n",
    "            tokenizer.model_max_length = 8192\n",
    "        case path if 'mistralai/mistral-7b-instruct-v0.3' in path:\n",
    "            tokenizer.model_max_length = 32768\n",
    "        case path if 'zephyr' in path:\n",
    "            tokenizer.model_max_length = 32768\n",
    "\n",
    "    if model_params.chat_template is not None:\n",
    "        tokenizer.chat_template = load_chat_template(model_params.chat_template)\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    print(tokenizer.truncation_side)\n",
    "    return tokenizer\n",
    "\n",
    "def load_chat_template(template_name):\n",
    "    chat_template = open(f'/nfs/staff-ssd/beyer/llm-quick-check/chat_templates/chat_templates/{template_name}.jinja').read()\n",
    "    chat_template = chat_template.replace('    ', '').replace('\\n', '')\n",
    "    return chat_template\n",
    "\n",
    "\n",
    "def check_tokenizer(tokenizer_id, cfg):\n",
    "    # model = AutoModelForCausalLM.from_pretrained(cfg.id, trust_remote_code=True)\n",
    "    modded_tokenizer = load_tokenizer(tokenizer_id, cfg)\n",
    "    cfg.chat_template = None\n",
    "    tokenizer = load_tokenizer(tokenizer_id, cfg)\n",
    "\n",
    "    # message =  [{'role': 'system', 'content': \"You're a helpful assistant.\"}, {'role': 'user', 'content': 'Hi!'}, {'role': 'assistant', 'content': 'Hi! How can I help you today?'},{'role': 'user', 'content': 'Hi!'}, {'role': 'assistant', 'content': 'Hi! How can I help you today?'}]\n",
    "    # message2 = [{'role': 'system', 'content': \"You're a helpful assistant.\"}, {'role': 'user', 'content': 'Hi!'}, {'role': 'assistant', 'content': 'Hi! How can I help you today?'},{'role': 'user', 'content': 'Hiadf!'}, {'role': 'assistant', 'content': 'Hi! How can asdfI help you today?'}]\n",
    "    # messages = [message, message2]\n",
    "    # try:\n",
    "    #     prompt_list = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    #     prompt_list2 = modded_tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    # except Exception as e:\n",
    "    #     prompt_list = tokenizer.apply_chat_template([m[1:] for m in messages], add_generation_prompt=True, tokenize=False)\n",
    "    #     prompt_list2 = modded_tokenizer.apply_chat_template([m[1:] for m in messages], add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "\n",
    "    # if prompt_list != prompt_list2:\n",
    "    #     print(tokenizer.name_or_path, 'differ1')\n",
    "    #     print(prompt_list)\n",
    "    #     print(prompt_list2)\n",
    "    #     # return False\n",
    "    # prompt_tokens = tokenizer(prompt_list, return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "    # prompt_tokens2 = modded_tokenizer(prompt_list2, return_tensors='pt', padding=True, add_special_tokens=False)\n",
    "\n",
    "    # m1 = model(**prompt_tokens)\n",
    "    # m2 = model(**prompt_tokens2)\n",
    "    # torch.no_grad()\n",
    "    # def compute_loss(logits, labels):\n",
    "    #     loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    #     shift_logits = logits[..., :-1, :].contiguous()\n",
    "    #     shift_labels = labels[..., 1:].contiguous()\n",
    "    #     loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)).view(labels.size(0), -1).detach()\n",
    "    #     non_padded_positions = shift_labels != tokenizer.pad_token_id\n",
    "    #     loss = [l[np] for l, np in zip(loss, non_padded_positions)]\n",
    "    #     # loss = [l.mean() for l in loss]\n",
    "    #     # print(loss)\n",
    "    #     return loss\n",
    "\n",
    "    # loss1 = compute_loss(m1.logits, prompt_tokens.input_ids)\n",
    "    # loss2 = compute_loss(m2.logits, prompt_tokens2.input_ids)\n",
    "    # if not all(l1.size() == l2.size() and torch.allclose(l1, l2) for l1, l2 in zip(loss1, loss2)):\n",
    "    #     plt.close()\n",
    "    #     fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n",
    "    #     max_len = max(len(loss1[0]), len(loss2[0]))\n",
    "    #     loss1_aligned = torch.zeros(max_len)\n",
    "    #     loss1_aligned[-len(loss1[0]):] = torch.tensor(loss1[0])\n",
    "    #     loss2_aligned = torch.zeros(max_len)\n",
    "    #     loss2_aligned[-len(loss2[0]):] = torch.tensor(loss2[0])\n",
    "    #     # Plotting the loss for the first message set\n",
    "    #     axs[0].plot(loss1_aligned, label='HF default')\n",
    "    #     axs[0].plot(loss2_aligned, label='chat_template')\n",
    "    #     axs[0].legend()\n",
    "    #     axs[0].set_title('Loss for first message set')\n",
    "\n",
    "    #     # Adding xticks with tokens for the first message set\n",
    "    #     tokens = tokenizer.convert_ids_to_tokens(prompt_tokens.input_ids[0])\n",
    "    #     axs[0].set_xticks(range(len(tokens)))\n",
    "    #     axs[0].set_xticklabels(tokens, rotation=90, fontsize=8)\n",
    "\n",
    "    #     # Plotting the loss for the second message set\n",
    "    #     axs[1].plot(loss1[1], label='HF default')\n",
    "    #     axs[1].plot(loss2[1], label='chat_template')\n",
    "    #     axs[1].legend()\n",
    "    #     axs[1].set_title('Loss for second message set')\n",
    "\n",
    "    #     # Adding xticks with tokens for the second message set\n",
    "    #     tokens2 = tokenizer.convert_ids_to_tokens(prompt_tokens.input_ids[1])\n",
    "    #     axs[1].set_xticks(range(len(tokens2)))\n",
    "    #     axs[1].set_xticklabels(tokens2, rotation=90, fontsize=8)\n",
    "\n",
    "    #     plt.tight_layout()\n",
    "    #     plt.show()\n",
    "\n",
    "    # if prompt_tokens.input_ids.size() != prompt_tokens2.input_ids.size() or not torch.allclose(prompt_tokens.input_ids, prompt_tokens2.input_ids):\n",
    "    #     print(tokenizer.name_or_path, 'differ2')\n",
    "    #     print(prompt_tokens.input_ids)\n",
    "    #     print(prompt_tokens2.input_ids)\n",
    "\n",
    "    return True\n",
    "for cfg in models:\n",
    "    try:\n",
    "        check_tokenizer(cfg.tokenizer_id, cfg)\n",
    "    except Exception as e:\n",
    "        print(cfg.tokenizer_id, e)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd02877ad23c4379b51958d7134030a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/gemma-2-2b-it eager\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b571beb6debd4145b76ae765c036dee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cfg \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mid, load_model_and_tokenizer(cfg\u001b[38;5;241m.\u001b[39mid, cfg)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation)\n",
      "File \u001b[0;32m/nfs/staff-ssd/beyer/llm-quick-check/src/io_utils.py:38\u001b[0m, in \u001b[0;36mload_model_and_tokenizer\u001b[0;34m(model_path, model_params)\u001b[0m\n\u001b[1;32m     32\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     33\u001b[0m         model_path,\n\u001b[1;32m     34\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m         quantization_config\u001b[38;5;241m=\u001b[39mquantization_config,\n\u001b[1;32m     36\u001b[0m     )\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     39\u001b[0m         model_path,\n\u001b[1;32m     40\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(torch, model_params\u001b[38;5;241m.\u001b[39mdtype),\n\u001b[1;32m     41\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mmodel_params\u001b[38;5;241m.\u001b[39mtrust_remote_code,\n\u001b[1;32m     42\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     43\u001b[0m         device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     44\u001b[0m     )\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_params\u001b[38;5;241m.\u001b[39mcompile:\n\u001b[1;32m     46\u001b[0m     model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(model)\n",
      "File \u001b[0;32m/nfs/staff-ssd/beyer/miniconda3/envs/std/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/nfs/staff-ssd/beyer/miniconda3/envs/std/lib/python3.12/site-packages/transformers/modeling_utils.py:4224\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4214\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4215\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4217\u001b[0m     (\n\u001b[1;32m   4218\u001b[0m         model,\n\u001b[1;32m   4219\u001b[0m         missing_keys,\n\u001b[1;32m   4220\u001b[0m         unexpected_keys,\n\u001b[1;32m   4221\u001b[0m         mismatched_keys,\n\u001b[1;32m   4222\u001b[0m         offload_index,\n\u001b[1;32m   4223\u001b[0m         error_msgs,\n\u001b[0;32m-> 4224\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[1;32m   4225\u001b[0m         model,\n\u001b[1;32m   4226\u001b[0m         state_dict,\n\u001b[1;32m   4227\u001b[0m         loaded_state_dict_keys,  \u001b[38;5;66;03m# XXX: rename?\u001b[39;00m\n\u001b[1;32m   4228\u001b[0m         resolved_archive_file,\n\u001b[1;32m   4229\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   4230\u001b[0m         ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[1;32m   4231\u001b[0m         sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[1;32m   4232\u001b[0m         _fast_init\u001b[38;5;241m=\u001b[39m_fast_init,\n\u001b[1;32m   4233\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m   4234\u001b[0m         device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   4235\u001b[0m         offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   4236\u001b[0m         offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[1;32m   4237\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[1;32m   4238\u001b[0m         hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[1;32m   4239\u001b[0m         keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   4240\u001b[0m         gguf_path\u001b[38;5;241m=\u001b[39mgguf_path,\n\u001b[1;32m   4241\u001b[0m         weights_only\u001b[38;5;241m=\u001b[39mweights_only,\n\u001b[1;32m   4242\u001b[0m     )\n\u001b[1;32m   4244\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4245\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/nfs/staff-ssd/beyer/miniconda3/envs/std/lib/python3.12/site-packages/transformers/modeling_utils.py:4727\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4723\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4724\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4725\u001b[0m                 )\n\u001b[1;32m   4726\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4727\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   4728\u001b[0m             model_to_load,\n\u001b[1;32m   4729\u001b[0m             state_dict,\n\u001b[1;32m   4730\u001b[0m             start_prefix,\n\u001b[1;32m   4731\u001b[0m             expected_keys,\n\u001b[1;32m   4732\u001b[0m             device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[1;32m   4733\u001b[0m             offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[1;32m   4734\u001b[0m             offload_index\u001b[38;5;241m=\u001b[39moffload_index,\n\u001b[1;32m   4735\u001b[0m             state_dict_folder\u001b[38;5;241m=\u001b[39mstate_dict_folder,\n\u001b[1;32m   4736\u001b[0m             state_dict_index\u001b[38;5;241m=\u001b[39mstate_dict_index,\n\u001b[1;32m   4737\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   4738\u001b[0m             hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[1;32m   4739\u001b[0m             is_safetensors\u001b[38;5;241m=\u001b[39mis_safetensors,\n\u001b[1;32m   4740\u001b[0m             keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   4741\u001b[0m             unexpected_keys\u001b[38;5;241m=\u001b[39munexpected_keys,\n\u001b[1;32m   4742\u001b[0m         )\n\u001b[1;32m   4743\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4745\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m/nfs/staff-ssd/beyer/miniconda3/envs/std/lib/python3.12/site-packages/transformers/modeling_utils.py:992\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    989\u001b[0m         param_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 992\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mset_module_kwargs)\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    994\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m/nfs/staff-ssd/beyer/miniconda3/envs/std/lib/python3.12/site-packages/accelerate/utils/modeling.py:416\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    414\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 416\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for cfg in models:\n",
    "    print(cfg.id, load_model_and_tokenizer(cfg.id, cfg)[0].config._attn_implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack string tokenizes to the same number of tokens across tokenziers (important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cfg \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m----> 2\u001b[0m     tok \u001b[38;5;241m=\u001b[39m load_tokenizer(cfg\u001b[38;5;241m.\u001b[39mtokenizer_id, cfg)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tok(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx x x x x x x x x x x x x x x x x x x x\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mshape, cfg\u001b[38;5;241m.\u001b[39mtokenizer_id)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "for cfg in models:\n",
    "    tok = load_tokenizer(cfg.tokenizer_id, cfg)\n",
    "    print(tok(\"x x x x x x x x x x x x x x x x x x x x\", return_tensors='pt', add_special_tokens=False).input_ids.shape, cfg.tokenizer_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLama2 is just weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698e34ec17724e14beb3d6ec3c5f317e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ca179eb27143f893863085f9068892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1aaa8826afd424fac08e1805aeb7c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f62e1a812134ef99dfc72da3d7267c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>[INST] Hi! [/INST] Hi! How can I help you today? </s><s>[INST] Hi! [/INST] Hi! How can I help you today? </s>', '<s>[INST] Hi! [/INST] Hi! How can I help you today? </s><s>[INST] Hiadf! [/INST] Hi! How can asdfI help you today? </s>']\n",
      "{'input_ids': tensor([[    1,     1,   518, 25580, 29962,  6324, 29991,   518, 29914, 25580,\n",
      "         29962,  6324, 29991,  1128,   508,   306,  1371,   366,  9826, 29973,\n",
      "         29871,     2,     1,   518, 25580, 29962,  6324, 29991,   518, 29914,\n",
      "         25580, 29962,  6324, 29991,  1128,   508,   306,  1371,   366,  9826,\n",
      "         29973, 29871,     2,     0,     0,     0,     0],\n",
      "        [    1,     1,   518, 25580, 29962,  6324, 29991,   518, 29914, 25580,\n",
      "         29962,  6324, 29991,  1128,   508,   306,  1371,   366,  9826, 29973,\n",
      "         29871,     2,     1,   518, 25580, 29962,  6324,   328, 29888, 29991,\n",
      "           518, 29914, 25580, 29962,  6324, 29991,  1128,   508,   408,  2176,\n",
      "         29902,  1371,   366,  9826, 29973, 29871,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[    1,     1,   518, 25580, 29962,  6324, 29991,   518, 29914, 25580,\n",
      "         29962, 29871,  6324, 29991,  1128,   508,   306,  1371,   366,  9826,\n",
      "         29973, 29871,     2,     1,   518, 25580, 29962,  6324, 29991,   518,\n",
      "         29914, 25580, 29962, 29871,  6324, 29991,  1128,   508,   306,  1371,\n",
      "           366,  9826, 29973, 29871,     2,     0,     0,     0,     0],\n",
      "        [    1,     1,   518, 25580, 29962,  6324, 29991,   518, 29914, 25580,\n",
      "         29962, 29871,  6324, 29991,  1128,   508,   306,  1371,   366,  9826,\n",
      "         29973, 29871,     2,     1,   518, 25580, 29962,  6324,   328, 29888,\n",
      "         29991,   518, 29914, 25580, 29962, 29871,  6324, 29991,  1128,   508,\n",
      "           408,  2176, 29902,  1371,   366,  9826, 29973, 29871,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]])}\n"
     ]
    }
   ],
   "source": [
    "message = [{'role': 'user', 'content': 'Hi!'}, {'role': 'assistant', 'content': 'Hi! How can I help you today?'},{'role': 'user', 'content': 'Hi!'}, {'role': 'assistant', 'content': 'Hi! How can I help you today?'}]\n",
    "message2 = [{'role': 'user', 'content': 'Hi!'}, {'role': 'assistant', 'content': 'Hi! How can I help you today?'},{'role': 'user', 'content': 'Hiadf!'}, {'role': 'assistant', 'content': 'Hi! How can asdfI help you today?'}]\n",
    "messages = [message, message2]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "prompt_list = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "print(prompt_list)\n",
    "prompt_tokens = tokenizer(prompt_list, return_tensors='pt', padding=True)\n",
    "\n",
    "def fix_llama2_tokens(inputs):\n",
    "    target_sequence = torch.tensor([518, 29914, 25580, 29962])\n",
    "    replacement_sequence = torch.tensor([518, 29914, 25580, 29962, 29871])\n",
    "    B = inputs['input_ids'].shape[0]\n",
    "    new_input_ids_list = []\n",
    "    new_attention_mask_list = []\n",
    "\n",
    "    for idx in range(B):\n",
    "        input_ids = inputs['input_ids'][idx]\n",
    "        attention_mask = inputs['attention_mask'][idx]\n",
    "\n",
    "        new_input_ids = []\n",
    "        new_attention_mask = []\n",
    "\n",
    "        idx_pos = 0\n",
    "        while idx_pos < len(input_ids):\n",
    "            # Check if the target sequence matches at the current position\n",
    "            if idx_pos <= len(input_ids) - len(target_sequence) and \\\n",
    "               torch.equal(input_ids[idx_pos:idx_pos+len(target_sequence)], target_sequence):\n",
    "                # Append the replacement sequence\n",
    "                new_input_ids.extend(replacement_sequence.tolist())\n",
    "                new_attention_mask.extend([1]*len(replacement_sequence))\n",
    "                idx_pos += len(target_sequence)\n",
    "            else:\n",
    "                # Append the current token\n",
    "                new_input_ids.append(input_ids[idx_pos].item())\n",
    "                new_attention_mask.append(attention_mask[idx_pos].item())\n",
    "                idx_pos += 1\n",
    "\n",
    "        new_input_ids_list.append(torch.tensor(new_input_ids, dtype=torch.long))\n",
    "        new_attention_mask_list.append(torch.tensor(new_attention_mask, dtype=torch.long))\n",
    "\n",
    "    # Find the maximum sequence length after replacements\n",
    "    max_length = max(len(seq) for seq in new_input_ids_list)\n",
    "\n",
    "    # Initialize new tensors with the maximum sequence length\n",
    "    new_input_ids_padded = torch.zeros((B, max_length), dtype=torch.long)\n",
    "    new_attention_mask_padded = torch.zeros((B, max_length), dtype=torch.long)\n",
    "\n",
    "    # Populate the new tensors with the updated sequences\n",
    "    for idx in range(B):\n",
    "        seq_len = len(new_input_ids_list[idx])\n",
    "        new_input_ids_padded[idx, :seq_len] = new_input_ids_list[idx]\n",
    "        new_attention_mask_padded[idx, :seq_len] = new_attention_mask_list[idx]\n",
    "\n",
    "    inputs['input_ids'] = new_input_ids_padded\n",
    "    inputs['attention_mask'] = new_attention_mask_padded\n",
    "    return inputs\n",
    "\n",
    "print(prompt_tokens)\n",
    "print(fix_llama2_tokens(prompt_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of prepare_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,   518, 25580, 29962]) tensor([22615]) tensor([29916,   921,   921,   921,   921]) tensor([  518, 29914, 25580, 29962, 29871]) tensor([18585, 29892,  1244, 29915, 29879])\n",
      "b\"<s> [INST] XXXx x x x x [/INST]  Sure, here's\"\n"
     ]
    }
   ],
   "source": [
    "target = \"Sure, here's\"\n",
    "from lm_utils import prepare_tokens\n",
    "prompt = 'XXX'\n",
    "attack = 'x x x x x'\n",
    "a,b,c,d,e = prepare_tokens(tokenizer, prompt, target, attack, 'suffix')\n",
    "print(a,b,c,d,e)\n",
    "print(tokenizer.decode(torch.cat([a,b,c,d,e], dim=-1)).encode('utf-8'))\n",
    "# print(tokenizer.decode(a))\n",
    "# print(tokenizer.decode(b))\n",
    "# print(tokenizer.decode(c))\n",
    "# print(tokenizer.decode(d))\n",
    "# print(tokenizer.decode(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where do we expect to see a specific token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b99004fc2f58445f8f7933883bd83772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da874080d254ce5ac51c0f952028789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86691ae8c18b45caaeecf146937552d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6194c0af29e4c7a84fb4c5e1cb4f415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c409e9edc84e9b89d48a2fd89ed1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25ea2fd75c14eee8528e0267276417d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a86b727962248c68184ff809b3cdeb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   518, 25580, 29962, 22615, 29916,   921,   921,   921,   921,\n",
      "           518, 29914, 25580, 29962, 29871, 18585, 29892,  1244, 29915, 29879,\n",
      "           263,  1950,  1234,   363,   278, 20285,   280, 29901,    13,    13,\n",
      "         22791, 29916,   921,   921,   921,   921,    13,    13,  1576,  1234,\n",
      "           338,   376,  6247,  6415,  1642,     2]])\n",
      "tensor([  263,  1950,  1234,   363,   278, 20285,   280, 29901,    13,    13,\n",
      "        22791, 29916,   921,   921,   921,   921,    13,    13,  1576,  1234,\n",
      "          338,   376,  6247,  6415,  1642,     2])\n",
      "a possible answer for the puzzle:\n",
      "\n",
      "XXXx x x x x\n",
      "\n",
      "The answer is \"XXIX\".</s>\n",
      "tensor(1159.1196, grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", trust_remote_code=True)\n",
    "template = torch.cat([a,b,c,d,e], dim=0).unsqueeze(0).to(model.device)\n",
    "\n",
    "gens = model.generate(template, max_new_tokens=100)\n",
    "print(gens)\n",
    "print(gens[0][template.size(1):])\n",
    "print(tokenizer.decode(gens[0][template.size(1):]))\n",
    "\n",
    "\n",
    "logits = model(template).logits\n",
    "loss = torch.nn.functional.cross_entropy(logits[0], template[0].roll(-1, 0), reduction='none')\n",
    "print(loss.mean().exp())\n",
    "plt.scatter(torch.arange(logits.size(1)), logits[0, :, 29871].detach().cpu())\n",
    "plt.xticks(torch.arange(logits.size(1)), [f\"{tokenizer.decode(logits.argmax(-1).cpu().numpy()[0][i:i+1])}|{tokenizer.decode(template[:,min(i+1, logits.size(1)-1)])}\" for i in torch.arange(logits.size(1))], rotation=90)\n",
    "plt.plot(loss.cpu().detach(), color='red', label='Loss')\n",
    "plt.xlabel('Token Position')\n",
    "plt.ylabel('Logit Value')\n",
    "plt.yscale('symlog')\n",
    "plt.title('Logits for Token ID 29871')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_utils import get_batched_losses\n",
    "\n",
    "token_list = [template[0], template[0][:7]]\n",
    "targets = [t.roll(-1, 0) for t in token_list]\n",
    "losses2 = get_batched_losses(model, targets, token_list=token_list, padding_side='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys tensor([    1,   518, 25580, 29962])\n",
      "prompt tensor([15043, 29892,   306,   626,   263,  9508, 29889])\n",
      "attack tensor([ 6824, 21004])\n",
      "post tensor([  518, 29914, 25580, 29962, 29871])\n",
      "target tensor([18585, 29892,  3646, 29889])\n"
     ]
    }
   ],
   "source": [
    "from lm_utils import prepare_tokens\n",
    "\n",
    "\n",
    "target = \"Sure, target.\"\n",
    "prompt = \"Hello, I am a prompt.\"\n",
    "attack = \"!!!!!\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')\n",
    "a,b,c,d,e = prepare_tokens(tokenizer, prompt, target, attack, 'suffix')\n",
    "print('sys', a)\n",
    "print('prompt', b)\n",
    "print('attack', c)\n",
    "print('post', d)\n",
    "print('target', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9def85f068a4b04a7227655fca42e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"qwen/Qwen2-7B-Instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"qwen/Qwen2-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from transformers import DynamicCache\n",
    "from lm_utils import prepare_tokens\n",
    "import time\n",
    "\n",
    "\n",
    "@torch.no_grad\n",
    "def get_batched_completions_ref(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    embedding_list=None,\n",
    "    token_list=None,\n",
    "    max_new_tokens: int = 256,\n",
    "    return_tokens=False,\n",
    "    padding_side='right',\n",
    ") -> list[str] | torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate completions for multiple prompts in a single batch.\n",
    "    KV-cache implemented & tested only for right padding right now.\n",
    "    Heavily tested across models to be close to individual generations.\n",
    "    This is far from trivial due to various padding (left/right) and masking issues.\n",
    "    The final function is still not identical to individual generations, but it is close.\n",
    "    The reason for this is that attention masks typically do not use -inf for masked tokens,\n",
    "    but instead use values like -65504 for float16. This can lead to small differences in the\n",
    "    final logits and thus the generated tokens.\n",
    "    We are much closer to individual generations than HF model.generate, which often\n",
    "    fails in mysterious ways for LLama & Qwen models.\n",
    "    Number of generations that are the same as single-batch:\n",
    "        Model Name                         This function    HF generate\n",
    "        cais/zephyr_7b_r2d2                      100/100        100/100\n",
    "        ContinuousAT/Llama-2-7B-CAT              100/100         39/100\n",
    "        ContinuousAT/Phi-CAT                      90/100         95/100\n",
    "        ContinuousAT/Zephyr-CAT                  100/100        100/100\n",
    "        HuggingFaceH4/zephyr-7b-beta                   -              -\n",
    "        google/gemma-2-2b-it                      55/100         56/100\n",
    "        meta-llama/Meta-Llama-3.1-8B-Instruct     62/100         11/100\n",
    "        meta-llama/Llama-2-7b-chat-hf             88/100         30/100\n",
    "        microsoft/Phi-3-mini-4k-instruct          53/100         50/100\n",
    "        mistralai/Mistral-7B-Instruct-v0.3        83/100         79/100\n",
    "        qwen/Qwen2-7B-Instruct                    78/100         19/100\n",
    "        ---------------------------------------------------------------\n",
    "        Total                                    809/1000      579/1000\n",
    "\n",
    "    Args:\n",
    "        model: A pretrained model.\n",
    "        tokenizer: A pretrained tokenizer.\n",
    "        embedding_list: list[torch.Tensor], optional\n",
    "            A list of embeddings for each prompt. Should not be padded and can be of different lengths.\n",
    "        token_list: list[torch.Tensor], optional\n",
    "            A list of tokens for each prompt. Should not be padded and can be of different lengths.\n",
    "        max_new_tokens: The maximum number of tokens to generate for each prompt.\n",
    "    Returns:\n",
    "        A list of completions for each prompt.\n",
    "    \"\"\"\n",
    "    if embedding_list is  None and token_list is None:\n",
    "        raise ValueError(\"Either embedding_list or token_list must be provided.\")\n",
    "    if embedding_list is not None:\n",
    "        assert all(e.ndim == 2 for e in embedding_list), \"Embeddings must be 2D.\"\n",
    "        embedding_list = [e.to(model.device) for e in embedding_list]\n",
    "    if token_list is not None:\n",
    "        assert all(t.ndim == 1 for t in token_list), \"Tokens must be 1D.\"\n",
    "        token_list = [t.to(model.device) for t in token_list]\n",
    "        embedding_list = [\n",
    "            model.get_input_embeddings()(t.unsqueeze(0))[0] for t in token_list\n",
    "        ]\n",
    "\n",
    "    B = len(embedding_list)\n",
    "    tokens = []\n",
    "    if padding_side == 'left':\n",
    "        # Add left padding\n",
    "        embeddings = pad_sequence(\n",
    "            [e.flip(0) for e in embedding_list], batch_first=True, padding_value=0\n",
    "        ).flip(1)\n",
    "        padded_embeddings = F.pad(embeddings, (0, 0, 0, max_new_tokens))\n",
    "        # Create attention mask and position ids\n",
    "        lengths = [\n",
    "            {\n",
    "                \"padding\": embeddings.size(1) - e.size(0),\n",
    "                \"generation\": max_new_tokens - e.size(0),\n",
    "            }\n",
    "            for e in embedding_list\n",
    "        ]\n",
    "        attention_mask = torch.stack(\n",
    "            [\n",
    "                torch.cat([torch.zeros(pl[\"padding\"]), torch.ones([pl[\"generation\"]])])\n",
    "                for pl in lengths\n",
    "            ]\n",
    "        ).to(model.device)\n",
    "        position_ids = torch.stack(\n",
    "            [\n",
    "                torch.cat([torch.zeros(pl[\"padding\"]), torch.arange(pl[\"generation\"])])\n",
    "                for pl in lengths\n",
    "            ]\n",
    "        ).long().to(model.device)\n",
    "        next_token_idx = embeddings.size(1)\n",
    "        for i in range(max_new_tokens):\n",
    "            outputs = model(\n",
    "                inputs_embeds=padded_embeddings[:, :next_token_idx],\n",
    "                attention_mask=attention_mask[:, :next_token_idx],\n",
    "                position_ids=position_ids[:, :next_token_idx],\n",
    "            )\n",
    "            next_tokens = outputs.logits.argmax(dim=-1)[torch.arange(B), -1]\n",
    "            padded_embeddings[torch.arange(B), next_token_idx] = (\n",
    "                model.get_input_embeddings()(next_tokens).detach()\n",
    "            )\n",
    "            tokens.append(next_tokens)\n",
    "            next_token_idx += 1\n",
    "    elif padding_side == 'right':\n",
    "        # Add right padding\n",
    "        embeddings = pad_sequence(\n",
    "            [e for e in embedding_list], batch_first=True, padding_value=0\n",
    "        )\n",
    "        padded_embeddings = F.pad(embeddings, (0, 0, 0, max_new_tokens))\n",
    "        next_token_idx = torch.tensor([e.size(0) for e in embedding_list])\n",
    "        for i in range(max_new_tokens):\n",
    "            t0 = time.time()\n",
    "            outputs = model(inputs_embeds=padded_embeddings[:, :next_token_idx.max()])\n",
    "            next_tokens = outputs.logits.argmax(dim=-1)[torch.arange(B), next_token_idx-1]\n",
    "            padded_embeddings[torch.arange(B), next_token_idx] = (\n",
    "                model.get_input_embeddings()(next_tokens).detach()\n",
    "            )\n",
    "            tokens.append(next_tokens)\n",
    "            next_token_idx += 1\n",
    "            torch.cuda.synchronize()\n",
    "            t1 = time.time()\n",
    "            print(i, t1-t0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown padding_side: {padding_side}\")\n",
    "\n",
    "    tokens = torch.stack(tokens, dim=0).T\n",
    "    if return_tokens:\n",
    "        return tokens\n",
    "    completion = tokenizer.batch_decode(tokens, skip_special_tokens=False)\n",
    "    completion = [c.split(tokenizer.eos_token)[0] for c in completion]\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Sup homie, how you doing?\"\n",
    "attack = \"Peter is super, right?\"\n",
    "target = \"Sure, target.\"\n",
    "a, b, c, d, e = prepare_tokens(tokenizer, prompt, target, attack, \"suffix\")\n",
    "token_list = [torch.cat(prepare_tokens(tokenizer, prompt, target, attack * i, \"suffix\")[:4], dim=0).to(model.device) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.05179595947265625\n",
      "1 0.046280622482299805\n",
      "2 0.04791116714477539\n",
      "3 0.046485185623168945\n",
      "4 0.04857969284057617\n",
      "5 0.04150819778442383\n",
      "6 0.030009746551513672\n",
      "7 0.02971339225769043\n",
      "8 0.030141115188598633\n",
      "9 0.03010392189025879\n",
      "10 0.030240535736083984\n",
      "11 0.030451536178588867\n",
      "12 0.030363082885742188\n",
      "13 0.030509233474731445\n",
      "14 0.03077983856201172\n",
      "15 0.03082752227783203\n",
      "16 0.03103160858154297\n",
      "17 0.031131267547607422\n",
      "18 0.03167295455932617\n",
      "19 0.031630516052246094\n",
      "20 0.040032386779785156\n",
      "21 0.03952288627624512\n",
      "22 0.03938579559326172\n",
      "23 0.039597511291503906\n",
      "24 0.03964519500732422\n",
      "25 0.039365530014038086\n",
      "26 0.03962135314941406\n",
      "27 0.03917860984802246\n",
      "28 0.03937649726867676\n",
      "29 0.04639720916748047\n",
      "30 0.03887486457824707\n",
      "31 0.03926897048950195\n",
      "32 0.04022336006164551\n",
      "33 0.04031252861022949\n",
      "34 0.03985404968261719\n",
      "35 0.03992891311645508\n",
      "36 0.04073524475097656\n",
      "37 0.04055976867675781\n",
      "38 0.04056859016418457\n",
      "39 0.04122114181518555\n",
      "40 0.0407872200012207\n",
      "41 0.04077911376953125\n",
      "42 0.04107975959777832\n",
      "43 0.041214942932128906\n",
      "44 0.04204511642456055\n",
      "45 0.04154062271118164\n",
      "46 0.041661977767944336\n",
      "47 0.04236197471618652\n",
      "48 0.041797637939453125\n",
      "49 0.041847944259643555\n",
      "50 0.042136192321777344\n",
      "51 0.0420382022857666\n",
      "52 0.045136451721191406\n",
      "53 0.045420169830322266\n",
      "54 0.04571866989135742\n",
      "55 0.04557228088378906\n",
      "56 0.04626750946044922\n",
      "57 0.046578407287597656\n",
      "58 0.04677724838256836\n",
      "59 0.04647183418273926\n",
      "60 0.046579837799072266\n",
      "61 0.04690718650817871\n",
      "62 0.0467529296875\n",
      "63 0.0466001033782959\n",
      "64 0.047173500061035156\n",
      "65 0.0471646785736084\n",
      "66 0.04697084426879883\n",
      "67 0.047405242919921875\n",
      "68 0.04924750328063965\n",
      "69 0.04868674278259277\n",
      "70 0.04928851127624512\n",
      "71 0.04951071739196777\n",
      "72 0.048975467681884766\n",
      "73 0.049257516860961914\n",
      "74 0.04940629005432129\n",
      "75 0.0492706298828125\n",
      "76 0.04941129684448242\n",
      "77 0.04955792427062988\n",
      "78 0.05014991760253906\n",
      "79 0.050145626068115234\n",
      "80 0.05004763603210449\n",
      "81 0.05093502998352051\n",
      "82 0.05094099044799805\n",
      "83 0.05062150955200195\n",
      "84 0.057579755783081055\n",
      "85 0.05806684494018555\n",
      "86 0.05783581733703613\n",
      "87 0.05845761299133301\n",
      "88 0.0592951774597168\n",
      "89 0.059847354888916016\n",
      "90 0.1481921672821045\n",
      "91 0.054351806640625\n",
      "92 0.05936884880065918\n",
      "93 0.05913352966308594\n",
      "94 0.0595545768737793\n",
      "95 0.059836387634277344\n",
      "96 0.05937933921813965\n",
      "97 0.05974173545837402\n",
      "98 0.059537410736083984\n",
      "99 0.05975675582885742\n",
      "100 0.059900522232055664\n",
      "101 0.060173988342285156\n",
      "102 0.06022143363952637\n",
      "103 0.060140132904052734\n",
      "104 0.06041741371154785\n",
      "105 0.061135053634643555\n",
      "106 0.06215524673461914\n",
      "107 0.06109023094177246\n",
      "108 0.061060428619384766\n",
      "109 0.10038876533508301\n",
      "110 0.05606722831726074\n",
      "111 0.0607457160949707\n",
      "112 0.06083512306213379\n",
      "113 0.06138467788696289\n",
      "114 0.061453819274902344\n",
      "115 0.061490774154663086\n",
      "116 0.07028913497924805\n",
      "117 0.0693354606628418\n",
      "118 0.06892180442810059\n",
      "119 0.06949806213378906\n",
      "120 0.06956648826599121\n",
      "121 0.06917309761047363\n",
      "122 0.06991910934448242\n",
      "123 0.07151961326599121\n",
      "124 0.07155990600585938\n",
      "125 0.06952500343322754\n",
      "126 0.07101869583129883\n",
      "127 0.07080316543579102\n",
      "128 0.07091450691223145\n",
      "129 0.07067537307739258\n",
      "130 0.07260537147521973\n",
      "131 0.0713200569152832\n",
      "132 0.07130599021911621\n",
      "133 0.0707249641418457\n",
      "134 0.0722205638885498\n",
      "135 0.07172846794128418\n",
      "136 0.07114243507385254\n",
      "137 0.07179975509643555\n",
      "138 0.12121963500976562\n",
      "139 0.06776618957519531\n",
      "140 0.07273459434509277\n",
      "141 0.07258987426757812\n",
      "142 0.07202911376953125\n",
      "143 0.07179832458496094\n",
      "144 0.07291674613952637\n",
      "145 0.07254433631896973\n",
      "146 0.07369589805603027\n",
      "147 0.07299304008483887\n",
      "148 0.07576966285705566\n",
      "149 0.07779431343078613\n",
      "150 0.07595944404602051\n",
      "151 0.07857775688171387\n",
      "152 0.07653522491455078\n",
      "153 0.07612156867980957\n",
      "154 0.07820820808410645\n",
      "155 0.07662439346313477\n",
      "156 0.07683944702148438\n",
      "157 0.07650637626647949\n",
      "158 0.07662034034729004\n",
      "159 0.0781700611114502\n",
      "160 0.07798099517822266\n",
      "161 0.07735943794250488\n",
      "162 0.07741880416870117\n",
      "163 0.07758021354675293\n",
      "164 0.07808136940002441\n",
      "165 0.0781559944152832\n",
      "166 0.08027148246765137\n",
      "167 0.07912683486938477\n",
      "168 0.0779578685760498\n",
      "169 0.07813048362731934\n",
      "170 0.07847166061401367\n",
      "171 0.0785369873046875\n",
      "172 0.07846713066101074\n",
      "173 0.07830333709716797\n",
      "174 0.07943058013916016\n",
      "175 0.13954424858093262\n",
      "176 0.07682418823242188\n",
      "177 0.07895421981811523\n",
      "178 0.08047652244567871\n",
      "179 0.08043265342712402\n",
      "180 0.09382009506225586\n",
      "181 0.09187984466552734\n",
      "182 0.0922398567199707\n",
      "183 0.09181523323059082\n",
      "184 0.09257984161376953\n",
      "185 0.09198331832885742\n",
      "186 0.09517812728881836\n",
      "187 0.09203648567199707\n",
      "188 0.0909273624420166\n",
      "189 0.09150123596191406\n",
      "190 0.09220480918884277\n",
      "191 0.09185028076171875\n",
      "192 0.09308218955993652\n",
      "193 0.09430217742919922\n",
      "194 0.09267926216125488\n",
      "195 0.09261465072631836\n",
      "196 0.09383559226989746\n",
      "197 0.09356880187988281\n",
      "198 0.0934445858001709\n",
      "199 0.09355354309082031\n",
      "200 0.09343695640563965\n",
      "201 0.09421014785766602\n",
      "202 0.09329795837402344\n",
      "203 0.09323811531066895\n",
      "204 0.09363102912902832\n",
      "205 0.0956583023071289\n",
      "206 0.09531021118164062\n",
      "207 0.165147066116333\n",
      "208 0.09003520011901855\n",
      "209 0.09501385688781738\n",
      "210 0.09615278244018555\n",
      "211 0.09658670425415039\n",
      "212 0.10329627990722656\n",
      "213 0.1038365364074707\n",
      "214 0.10558199882507324\n",
      "215 0.10345244407653809\n",
      "216 0.10580682754516602\n",
      "217 0.10606813430786133\n",
      "218 0.10432267189025879\n",
      "219 0.1059110164642334\n",
      "220 0.10426568984985352\n",
      "221 0.10437464714050293\n",
      "222 0.10501766204833984\n",
      "223 0.10575699806213379\n",
      "224 0.10631465911865234\n",
      "225 0.10612034797668457\n",
      "226 0.1062479019165039\n",
      "227 0.10605955123901367\n",
      "228 0.10780739784240723\n",
      "229 0.10556793212890625\n",
      "230 0.1069345474243164\n",
      "231 0.10733199119567871\n",
      "232 0.10700082778930664\n",
      "233 0.10702633857727051\n",
      "234 0.10675740242004395\n",
      "235 0.19590139389038086\n",
      "236 0.10299301147460938\n",
      "237 0.10834121704101562\n",
      "238 0.10795092582702637\n",
      "239 0.10768532752990723\n",
      "240 0.10859537124633789\n",
      "241 0.10735344886779785\n",
      "242 0.10729408264160156\n",
      "243 0.10931921005249023\n",
      "244 0.11134552955627441\n",
      "245 0.10999250411987305\n",
      "246 0.11000895500183105\n",
      "247 0.11079263687133789\n",
      "248 0.11072325706481934\n",
      "249 0.11108899116516113\n",
      "250 0.11162304878234863\n",
      "251 0.11101651191711426\n",
      "252 0.11262917518615723\n",
      "253 0.11199498176574707\n",
      "254 0.11119818687438965\n",
      "255 0.11268472671508789\n"
     ]
    }
   ],
   "source": [
    "c_ref = get_batched_completions_ref(\n",
    "    model, tokenizer, token_list=[t for t in token_list], max_new_tokens=256, padding_side=\"right\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hello! I'm an AI assistant, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\", \"Hello! As an AI, I don't have feelings, but I'm here and ready to assist you. Regarding Peter, I don't have information about an individual named Peter unless you've mentioned him in previous context or I've interacted with him before. How can I assist you further?\", \"As an AI, I don't have feelings, but I'm here and ready to assist you. If you have any questions or need help with something, feel free to ask!\", \"As an AI, I don't have feelings, but I'm here and ready to assist you. If you have any questions or need help with something, feel free to ask!\"]\n",
      "[\"Hey! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you with any questions or tasks you have. How can I assist you today?\", \"Hello! As an AI, I don't have feelings, but I'm here and ready to assist you. Regarding Peter, I don't have information about an individual named Peter unless you've mentioned him in previous context or I've interacted with him before. How can I assist you further?\", \"As an AI, I don't have feelings, but I'm here and ready to assist you. If you have any questions or need help with something, feel free to ask!\", \"Hello! I'm an AI and don't have feelings, but I'm here and ready to help you. Regarding Peter, I don't have information about an individual named Peter unless you've provided context in your previous questions or statements. If you're referring to a specific person, could you please provide more details?\"]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from transformers import HybridCache\n",
    "import logging\n",
    "\n",
    "@torch.no_grad\n",
    "def get_batched_completions_cached(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    embedding_list=None,\n",
    "    token_list=None,\n",
    "    max_new_tokens: int = 256,\n",
    "    return_tokens=False,\n",
    "    padding_side=\"right\",\n",
    "    use_cache=False,\n",
    ") -> list[str] | torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate completions for multiple prompts in a single batch.\n",
    "    No KV-cache for now.\n",
    "    Heavily tested across models to be close to individual generations.\n",
    "    This is far from trivial due to various padding (left/right) and masking issues.\n",
    "    The final function is still not identical to individual generations, but it is close.\n",
    "    The reason for this is probably that attention masks typically don't use -inf for\n",
    "    masked tokens, but instead have values like -65504 for float16.\n",
    "    This can lead to small differences in the final logits and thus the generated tokens.\n",
    "    We are much closer to individual generations than HF model.generate, which often\n",
    "    fails in mysterious ways for LLama & Qwen models.\n",
    "    Number of generations that are the same as single-batch:\n",
    "        Model Name                         This function    HF generate\n",
    "        cais/zephyr_7b_r2d2                      100/100        100/100\n",
    "        ContinuousAT/Llama-2-7B-CAT              100/100         39/100\n",
    "        ContinuousAT/Phi-CAT                      90/100         95/100\n",
    "        ContinuousAT/Zephyr-CAT                  100/100        100/100\n",
    "        google/gemma-2-2b-it                      55/100         56/100\n",
    "        meta-llama/Meta-Llama-3.1-8B-Instruct     62/100         11/100\n",
    "        meta-llama/Llama-2-7b-chat-hf             88/100         30/100\n",
    "        microsoft/Phi-3-mini-4k-instruct          53/100         50/100\n",
    "        mistralai/Mistral-7B-Instruct-v0.3        83/100         79/100\n",
    "        qwen/Qwen2-7B-Instruct                    78/100         19/100\n",
    "        ---------------------------------------------------------------\n",
    "        Total                                   809/1000       579/1000\n",
    "\n",
    "    Args:\n",
    "        model: A pretrained model.\n",
    "        tokenizer: A pretrained tokenizer.\n",
    "        embedding_list: list[torch.Tensor], optional\n",
    "            A list of embeddings for each prompt. Should not be padded and can be of different lengths.\n",
    "        token_list: list[torch.Tensor], optional\n",
    "            A list of tokens for each prompt. Should not be padded and can be of different lengths.\n",
    "        max_new_tokens: The maximum number of tokens to generate for each prompt.\n",
    "    Returns:\n",
    "        A list of completions for each prompt.\n",
    "    \"\"\"\n",
    "    if embedding_list is None and token_list is None:\n",
    "        raise ValueError(\"Either embedding_list or token_list must be provided.\")\n",
    "    if embedding_list is not None:\n",
    "        assert all(e.ndim == 2 for e in embedding_list), \"Embeddings must be 2D.\"\n",
    "        embedding_list = [e.to(model.device) for e in embedding_list]\n",
    "    if token_list is not None:\n",
    "        assert all(t.ndim == 1 for t in token_list), \"Tokens must be 1D.\"\n",
    "        token_list = [t.to(model.device) for t in token_list]\n",
    "        embedding_list = [\n",
    "            model.get_input_embeddings()(t.unsqueeze(0))[0] for t in token_list\n",
    "        ]\n",
    "    # TODO: Implement KV-caching for Gemma\n",
    "    if use_cache and model.name_or_path == \"google/gemma-2-2b-it\":\n",
    "        logging.warning(\"KV-cache not implemented for Gemma 2. Disabling cache.\")\n",
    "        use_cache = False\n",
    "\n",
    "\n",
    "    B = len(embedding_list)\n",
    "    tokens = []\n",
    "    if padding_side == \"left\":\n",
    "        if use_cache:\n",
    "            raise NotImplementedError(\"KV-cache not implemented for left padding.\")\n",
    "        # Add left padding\n",
    "        embeddings = pad_sequence(\n",
    "            [e.flip(0) for e in embedding_list], batch_first=True, padding_value=0\n",
    "        ).flip(1)\n",
    "        padded_embeddings = F.pad(embeddings, (0, 0, 0, max_new_tokens))\n",
    "        # Create attention mask and position ids\n",
    "        lengths = [\n",
    "            {\n",
    "                \"padding\": embeddings.size(1) - e.size(0),\n",
    "                \"generation\": max_new_tokens - e.size(0),\n",
    "            }\n",
    "            for e in embedding_list\n",
    "        ]\n",
    "        attention_mask = torch.stack(\n",
    "            [\n",
    "                torch.cat([torch.zeros(pl[\"padding\"]), torch.ones([pl[\"generation\"]])])\n",
    "                for pl in lengths\n",
    "            ]\n",
    "        ).to(model.device)\n",
    "        position_ids = (\n",
    "            torch.stack(\n",
    "                [\n",
    "                    torch.cat([torch.zeros(pl[\"padding\"]), torch.arange(pl[\"generation\"])])\n",
    "                    for pl in lengths\n",
    "                ]\n",
    "            )\n",
    "            .long()\n",
    "            .to(model.device)\n",
    "        )\n",
    "        next_token_idx = embeddings.size(1)\n",
    "        for i in range(max_new_tokens):\n",
    "            outputs = model(\n",
    "                inputs_embeds=padded_embeddings[:, :next_token_idx],\n",
    "                attention_mask=attention_mask[:, :next_token_idx],\n",
    "                position_ids=position_ids[:, :next_token_idx],\n",
    "            )\n",
    "            next_tokens = outputs.logits.argmax(dim=-1)[torch.arange(B), -1]\n",
    "            padded_embeddings[torch.arange(B), next_token_idx] = (\n",
    "                model.get_input_embeddings()(next_tokens).detach()\n",
    "            )\n",
    "            tokens.append(next_tokens)\n",
    "            next_token_idx += 1\n",
    "    elif padding_side == \"right\":\n",
    "        # Add right padding\n",
    "        embeddings = pad_sequence(\n",
    "            [e for e in embedding_list], batch_first=True, padding_value=0\n",
    "        )\n",
    "        padded_embeddings = F.pad(embeddings, (0, 0, 0, max_new_tokens))\n",
    "        next_token_idx = torch.tensor([e.size(0) for e in embedding_list])\n",
    "\n",
    "        if use_cache:\n",
    "            # Fill prefix cache\n",
    "            past_key_values = DynamicCache()\n",
    "            if next_token_idx.min() > 1:\n",
    "                model(\n",
    "                    inputs_embeds=padded_embeddings[:, : next_token_idx.min() - 1],\n",
    "                    past_key_values=past_key_values,\n",
    "                    use_cache=use_cache,\n",
    "                )\n",
    "            for i in range(max_new_tokens):\n",
    "                # Caching with right padding is a bit tricky:\n",
    "                # Because of right padding, we have to feed more than one token to the\n",
    "                # model at each forward pass :(.\n",
    "                # Instead, we feed a 'window' from the last token of the shortest prompt\n",
    "                # to the last token of the longest prompt.\n",
    "                # This means that caching works best if all sequences are of similar length.\n",
    "                outputs = model(\n",
    "                    inputs_embeds=padded_embeddings[:, next_token_idx.min() - 1 : next_token_idx.max()],\n",
    "                    past_key_values=past_key_values,\n",
    "                    use_cache=use_cache,\n",
    "                )\n",
    "                next_tokens = outputs.logits.argmax(dim=-1)[\n",
    "                    torch.arange(B), next_token_idx - next_token_idx.min()\n",
    "                ]\n",
    "                padded_embeddings[torch.arange(B), next_token_idx] = (\n",
    "                    model.get_input_embeddings()(next_tokens).detach()\n",
    "                )\n",
    "                # have to manually crop the past_key_values to the correct length\n",
    "                # since we only add a single step at a time\n",
    "                for j in range(len(past_key_values.key_cache)):\n",
    "                    past_key_values.key_cache[j] = past_key_values.key_cache[j][..., : next_token_idx.min(), :]\n",
    "                    past_key_values.value_cache[j] = past_key_values.value_cache[j][..., : next_token_idx.min(), :]\n",
    "                tokens.append(next_tokens)\n",
    "                next_token_idx += 1\n",
    "        else:\n",
    "            for i in range(max_new_tokens):\n",
    "                outputs = model(\n",
    "                    inputs_embeds=padded_embeddings[:, :next_token_idx.max()]\n",
    "                )\n",
    "                next_tokens = outputs.logits.argmax(dim=-1)[torch.arange(B), next_token_idx - 1]\n",
    "                padded_embeddings[torch.arange(B), next_token_idx] = (\n",
    "                    model.get_input_embeddings()(next_tokens).detach()\n",
    "                )\n",
    "                tokens.append(next_tokens)\n",
    "                next_token_idx += 1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown padding_side: {padding_side}\")\n",
    "\n",
    "    tokens = torch.stack(tokens, dim=0).T\n",
    "    if return_tokens:\n",
    "        return tokens\n",
    "    completion = tokenizer.batch_decode(tokens, skip_special_tokens=False)\n",
    "    completion = [c.split(tokenizer.eos_token)[0] for c in completion]\n",
    "    return completion\n",
    "\n",
    "\n",
    "c_cached = get_batched_completions_cached(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    token_list=[t for t in token_list],\n",
    "    max_new_tokens=256,\n",
    "    padding_side=\"right\",\n",
    "    use_cache=True,\n",
    ")\n",
    "print(c_ref)\n",
    "print(c_cached)\n",
    "print(c_ref == c_cached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "std",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
